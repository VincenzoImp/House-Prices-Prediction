{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopy.distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-processing\n",
    "     \n",
    "     \n",
    "## 2.1 Clean and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5434/4245473066.py:3: DtypeWarning: Columns (7,9,11,13,17,18,19,22,23,24,25,26,28,29,30,32,33,34,35,36,37,38,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,74,77,80,82,83,84,85,86,87,90,91,92,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  poi_df = pd.read_csv('data/poi.csv')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "poi_df = pd.read_csv('data/poi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_df = poi_df[['lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = train_df.iloc[0].latitude, train_df.iloc[0].longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat = 52.406374\n",
    "lon = 16.9251681\n",
    "series = poi_df.apply(lambda x: geopy.distance.distance((lat, lon), (x['lat'], x['lon'])).km, axis=1)\n",
    "series[series < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series[series < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_distance_km(lat1, lon1, lat2, lon2):\n",
    "    return geopy.distance.distance((lat1, lon1), (lat2, lon2)).km\n",
    "lat1, lon1 = 1, 1\n",
    "lat2, lon2 = 1, 1\n",
    "compute_distance_km(lat1, lon1, lat2, lon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(lat, lon, poi_df):\n",
    "    series = poi_df.apply(lambda x: geopy.distance.distance((lat, lon), (x['lat'], x['lon'])).km, axis=1)\n",
    "    series.sort_values()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.15 TiB for an array with shape (157912453924,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39;49mmerge(poi_df, poi_df, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcross\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/core/reshape/merge.py:124\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m         validate\u001b[39m=\u001b[39mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result(copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/core/reshape/merge.py:773\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindicator:\n\u001b[1;32m    771\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indicator_pre_merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright)\n\u001b[0;32m--> 773\u001b[0m join_index, left_indexer, right_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_info()\n\u001b[1;32m    775\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_and_concat(\n\u001b[1;32m    776\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    777\u001b[0m )\n\u001b[1;32m    778\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_type)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1026\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     join_index, right_indexer, left_indexer \u001b[39m=\u001b[39m _left_join_on_index(\n\u001b[1;32m   1023\u001b[0m         right_ax, left_ax, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1025\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m     (left_indexer, right_indexer) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_indexers()\n\u001b[1;32m   1028\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index:\n\u001b[1;32m   1029\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1000\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_join_indexers\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[1;32m    999\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[39mreturn\u001b[39;00m get_join_indexers(\n\u001b[1;32m   1001\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleft_join_keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_join_keys, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort, how\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1610\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m join_func \u001b[39m=\u001b[39m {\n\u001b[1;32m   1601\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39minner_join,\n\u001b[1;32m   1602\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39mleft_outer_join,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39mfull_outer_join,\n\u001b[1;32m   1607\u001b[0m }[how]\n\u001b[1;32m   1609\u001b[0m \u001b[39m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[0;32m-> 1610\u001b[0m \u001b[39mreturn\u001b[39;00m join_func(lkey, rkey, count, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/site-packages/pandas/_libs/join.pyx:48\u001b[0m, in \u001b[0;36mpandas._libs.join.inner_join\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.15 TiB for an array with shape (157912453924,) and data type int64"
     ]
    }
   ],
   "source": [
    "train_df['poi'] = train_df[['lat', 'lon']].apply(lambda x: foo(x['lat'], x['lon'], poi_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279.35290160430094"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "geopy.distance.geodesic(coords_1, coords_2).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x, y\n",
    "df = df.drop('url', axis=1, errors='ignore')\n",
    "x = df.drop('shares', axis=1)\n",
    "y = df.shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# compute feature_importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(x, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature_importances\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(x.columns[sorted_idx], rf.feature_importances_[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove features with feature_importances < 0.01\n",
    "for importance, feature in zip(rf.feature_importances_, x.columns):\n",
    "    if importance < 0.02:\n",
    "        x = x.drop(feature, axis=1)\n",
    "\n",
    "# compute feature_importances again\n",
    "rf.fit(x, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot feature_importances again\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(x.columns[sorted_idx], rf.feature_importances_[sorted_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 2.3\n",
    "On the entire dataset, a random forest was used to recognize the most relevant features. This enabled the classification of the importance of the characteristics. Due to a large number of features available, it was decided to consider only the most important. As a result, the features less important than 0.02 have been removed. At the end of this operation, there are 18 features left, accounting for roughly one-third of the total. The importance of the remaining features has been recalculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Model Selection (up to 8.2 of 11.2  points)\n",
    "In this part of the challenge you are requested to perform all the necessary steps required in order to design a full fledged classification task on the <b>shares</b> feature.\n",
    "\n",
    "You are requested to perform the following steps having in mind the following: \n",
    "\n",
    "1) the dataset must be properly splitted to perform crossvalidation \n",
    "\n",
    "2) when required, features must be properly encoded\n",
    "\n",
    "3) in order to simplify the problem the target feature can be dicretized <b>(number of classes must be >=5)</b> ;\n",
    "\n",
    "4) for model selection you are requested to consider: \n",
    "\n",
    "- Decision Trees\n",
    "\n",
    "- Support Vector Machines;\n",
    "\n",
    "- An ensamble methodology;\n",
    "\n",
    "- MLPNs.\n",
    "\n",
    "5) hyper-parameter tuning <b>must</b> be performed and discussed;\n",
    "\n",
    "6) apply standardizion and normalization when appropriate;\n",
    "\n",
    "7) remember to use an appropriate evaluation setting (cross-fold etc..)\n",
    "\n",
    "8) describe the measures adopted for the evaluation and discuss the results;\n",
    "\n",
    "9) provide a discussion of the model selection, where you describe the differences in terms of performance and explains the root causes;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create `new_df` which contains only the features chosen to train the models and 'shares'. \n",
    "The value `entry_number_step1` represents the number of samples considered at this point in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "columns = []\n",
    "for col in df.columns:\n",
    "    if col not in x.columns and col != 'shares':\n",
    "        new_df = new_df.drop(col, axis=1, errors='ignore')\n",
    "    else:\n",
    "        columns.append(col)\n",
    "entry_number_step1 = new_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i plot hist and hist2d for every feature to see the distribution of the data. Some features have distinct outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    get_hist(new_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following script, I eliminate the outliers. I also tried the other way (with the commented scripts) but in the end, I found it better to eliminate the outliers via zscore. It is reasonable to think that the data at our disposal may have some noise. It is better to remove it for better evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = zscore(new_df)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 5).all(axis=1)\n",
    "new_df = new_df[filtered_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I rerun the plot of hist and hist2d for each feature noting some differences. Without the outliers, the distributions are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    get_hist(new_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples eliminated is 1875, and the number of samples remaining is 37638. I believe it is correct to remove this number of samples, thereby removing data noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entry_number_step2 = new_df.shape[0]\n",
    "print('Samples before dropping outliers: {}'.format(entry_number_step1))\n",
    "print('Samples after dropping outliers: {}'.format(entry_number_step2))\n",
    "print('Samples removed:',format(entry_number_step1-entry_number_step2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features (`x`) are scaled, and 'shares' (`y`) is discretized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "x = new_df.drop('shares', axis=1)\n",
    "y = new_df.shares\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_scal = scaler.fit_transform(x)\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n",
    "y_disc = discretizer.fit_transform(y.array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then proceed to divide the dataset into `x_train`, `y_train`. `x_test`, `y_test`. I will only use the first two to train the models. and I will use the last two only and exclusively to test the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scal, y_disc, shuffle=True, stratify=y_disc, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the dictionary of models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = {\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'BOOST': AdaBoostClassifier(random_state=42),\n",
    "    'RND_FOREST': RandomForestClassifier(random_state=42),\n",
    "    'MLPN': MLPClassifier(hidden_layer_sizes=(9,4), random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the three useful functions to train and evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def fit_models(models, x_train, y_train):\n",
    "    for name in models.keys():\n",
    "        print('fitting {}'.format(name))\n",
    "        models[name].fit(x_train, y_train.ravel())\n",
    "        print('done')\n",
    "    return\n",
    "    \n",
    "def evaluate_models(models, x_test, y_test, x_scal, y_disc):\n",
    "    for name, model in models.items():\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print('name: {}'.format(name))\n",
    "        print()\n",
    "        print('params:')\n",
    "        print(model.get_params())\n",
    "        print()\n",
    "        y_pred = model.predict(x_test)\n",
    "        report = classification_report(y_test, y_pred, zero_division=0, target_names=['class0', 'class1', 'class2', 'class3', 'class4'])\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cv_accuracy = cross_val_score(model, x_scal, y_disc.ravel(), n_jobs=-1, scoring='accuracy')\n",
    "        cv_f1_macro = cross_val_score(model, x_scal, y_disc.ravel(), n_jobs=-1, scoring='f1_macro')\n",
    "        print('report:')\n",
    "        print(report)\n",
    "        print()\n",
    "        print('confusion_matrix:')\n",
    "        print(cm)\n",
    "        print()\n",
    "        print('cross_val_score (accuracy):')\n",
    "        print(cv_accuracy)\n",
    "        print(\"%0.4f (+/- %0.4f)\" % (cv_accuracy.mean(), cv_accuracy.std() * 2))\n",
    "        print()\n",
    "        print('cross_val_score (f1_macro):')\n",
    "        print(cv_f1_macro)\n",
    "        print(\"%0.4f (+/- %0.4f)\" % (cv_f1_macro.mean(), cv_f1_macro.std() * 2))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models, I print the dimensions of the train and test dataset. it is useful to stick to a fixed proportion to correctly perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = round((x_train.shape[0]/(x_train.shape[0]+x_test.shape[0]))*100, 2)\n",
    "print('train set dimention: {} ({}%)'.format(x_train.shape[0], train_perc))\n",
    "print('test set dimention: {} ({}%)'.format(x_test.shape[0], 100-train_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models(models, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_models(models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary evaluation:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> name </th><th>accuracy</th><th>f1macro</th><th>cv_accuracy</th><th>cv_f1macro</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DT</td><td>0.66</td><td>0.21</td><td>0.4071 (+/- 0.3962)</td><td>0.1513 (+/- 0.1054)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVM</td><td>0.80</td><td>0.18</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BOOST</td><td>0.80</td><td>0.18</td><td>0.7957 (+/- 0.0037)</td><td>0.1783 (+/- 0.0023)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RND_FOREST</td><td>0.80</td><td>0.18</td><td>0.6614 (+/- 0.5165)</td><td>0.1570 (+/- 0.1051)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MLPN</td><td>0.80</td><td>0.18</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "It was discovered that the unbalanced dataset resulted in excellent accuracy but a very poor f1_macro. This is due to the imbalanced dataset. We want to correctly predict all classes, we cannot predict exactly class 0 only. for this we want to maximize f1_macro while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the models have been trained with a large amount of class 0 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the intent is to balance the classes. target:\n",
    "- class 0: 1/3 samples\n",
    "- class 1: 1/4 samples\n",
    "- class 2: 1/5 samples\n",
    "- class 3: 1/6 samples\n",
    "- class 4: 1/7 samples\n",
    "\n",
    "the distribution totals 1.0928, it is tolerable, we will have just over 2/3 of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = min(np.count_nonzero(y_train == 0), int((1/3)*y_train.shape[0]))\n",
    "c1 = min(np.count_nonzero(y_train == 1), int((1/4)*y_train.shape[0]))\n",
    "c2 = min(np.count_nonzero(y_train == 2), int((1/5)*y_train.shape[0]))\n",
    "c3 = min(np.count_nonzero(y_train == 3), int((1/6)*y_train.shape[0]))\n",
    "c4 = min(np.count_nonzero(y_train == 4), int((1/7)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use RandomUnderSampler to undersampling classes that contain more samples than desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={0:c0, 1:c1, 2:c2, 3:c3, 4:c4}, random_state=42)\n",
    "x_train_under, y_train_under = rus.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_under_dim = x_train_under.shape[0]\n",
    "print('Samples before undersampling: {}'.format(x_train.shape[0]))\n",
    "print('Samples after undersampling: {}'.format(x_train_under_dim))\n",
    "print('Samples removed: {}'.format(x_train.shape[0]-x_train_under_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_under, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SMOTE to oversampling classes that contain less samples than desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = max(np.count_nonzero(y_train_under == 0), int((1/3)*y_train.shape[0]))\n",
    "c1 = max(np.count_nonzero(y_train_under == 1), int((1/4)*y_train.shape[0]))\n",
    "c2 = max(np.count_nonzero(y_train_under == 2), int((1/5)*y_train.shape[0]))\n",
    "c3 = max(np.count_nonzero(y_train_under == 3), int((1/6)*y_train.shape[0]))\n",
    "c4 = max(np.count_nonzero(y_train_under == 4), int((1/7)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(k_neighbors=3, n_jobs=-1, sampling_strategy={0:c0,1:c1,2:c2,3:c3,4:c4}, random_state=42)\n",
    "x_train_over, y_train_over = sm.fit_resample(x_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_number_step4 = x_train_over.shape[0]\n",
    "print('Samples before oversampling: {}'.format(x_train_under_dim))\n",
    "print('Samples after oversampling: {}'.format(entry_number_step4))\n",
    "print('Samples added: {}'.format(entry_number_step4-x_train_under_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the samples are balanced as originally planned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_over, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plot the distribution of the features following the new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(columns):\n",
    "    if col != 'shares':\n",
    "        to_plot = x_train_over[:,i]\n",
    "    else:\n",
    "        to_plot = y_train_over\n",
    "    min_value = to_plot.min()\n",
    "    max_value = to_plot.max()\n",
    "    b = 100\n",
    "    plt.hist(to_plot, bins=b)\n",
    "    plt.title('histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(col, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.hist2d(pd.Series(np.array([i for i in range(to_plot.shape[0])])), to_plot, bins=b, norm = colors.LogNorm())\n",
    "    plt.title('2D histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(col, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = round((x_train_over.shape[0]/(x_train_over.shape[0]+x_test.shape[0]))*100, 2)\n",
    "print('train set dimention: {} ({}%)'.format(x_train_over.shape[0], train_perc))\n",
    "print('test set dimention: {} ({}%)'.format(x_test.shape[0], 100-train_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fit and evaluate models with 2/3 of training set and 1/3 of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit_models(models, x_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_models(models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary evaluation:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> name </th><th>accuracy</th><th>f1macro</th><th>cv_accuracy</th><th>cv_f1macro</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DT</td><td>0.49</td><td>0.20</td><td>0.4071 (+/- 0.3962)</td><td>0.1513 (+/- 0.1054)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVM</td><td>0.65</td><td>0.25</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BOOST</td><td>0.59</td><td>0.23</td><td>0.7957 (+/- 0.0037)</td><td>0.1783 (+/- 0.0023)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RND_FOREST</td><td>0.70</td><td>0.26</td><td>0.6614 (+/- 0.5165)</td><td>0.1570 (+/- 0.1051)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MLPN</td><td>0.65</td><td>0.22</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "With under/over sampling a better f1_macro is noted, while maintaining good accuracy. To choose the best model I considered these values in order: cv_f1macro, cv_accuracy, f1_macro, accuracy. I selected SVC as the best model and I proceed with the hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define function for model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_model(model, param_grid, scoring, x_train, y_train, grid_jobs):\n",
    "    print('tuning...')\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, verbose=1, n_jobs=grid_jobs)\n",
    "    clf.fit(x_train, y_train.ravel())\n",
    "    print('done')\n",
    "    print()\n",
    "    print(\"Best: %f using %s\" % (clf.best_score_, clf.best_params_))\n",
    "    best_params = clf.best_params_.copy()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perform tuning on half of the test dataset, or one-third of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = min(np.count_nonzero(y_train == 0), int((1/6)*y_train.shape[0]))\n",
    "c1 = min(np.count_nonzero(y_train == 1), int((1/8)*y_train.shape[0]))\n",
    "c2 = min(np.count_nonzero(y_train == 2), int((1/10)*y_train.shape[0]))\n",
    "c3 = min(np.count_nonzero(y_train == 3), int((1/12)*y_train.shape[0]))\n",
    "c4 = min(np.count_nonzero(y_train == 4), int((1/14)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy={0:c0, 1:c1, 2:c2, 3:c3, 4:c4}, random_state=42)\n",
    "x_tuning, y_tuning = rus.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = max(np.count_nonzero(y_tuning == 0), int((1/6)*y_train.shape[0]))\n",
    "c1 = max(np.count_nonzero(y_tuning == 1), int((1/8)*y_train.shape[0]))\n",
    "c2 = max(np.count_nonzero(y_tuning == 2), int((1/10)*y_train.shape[0]))\n",
    "c3 = max(np.count_nonzero(y_tuning == 3), int((1/12)*y_train.shape[0]))\n",
    "c4 = max(np.count_nonzero(y_tuning == 4), int((1/14)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(k_neighbors=3, n_jobs=-1, sampling_strategy={0:c0,1:c1,2:c2,3:c3,4:c4}, random_state=42)\n",
    "x_tuning, y_tuning = sm.fit_resample(x_tuning, y_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_tuning, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set param grid for tuning model with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_param_grid = {\n",
    "    'C': [0.5, 1],\n",
    "    'kernel': ['rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'decision_function_shape': ['ovo', 'ovr'],\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tune model by accuracy and i find `best_accuracy_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy_params = tune_model(SVC(), SVM_param_grid, 'accuracy', x_tuning, y_tuning, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tune model by f1_macro and i find `best_f1macro_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1macro_params = tune_model(SVC(), SVM_param_grid, 'f1_macro',  x_tuning, y_tuning, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fit and evaluate the two models with the best parameters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = {\n",
    "    'SVM_accuracy': SVC(**best_accuracy_params),\n",
    "    'SVM_f1macro': SVC(**best_f1macro_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models(tuned_models, x_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(tuned_models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 3\n",
    "The outliers were removed from the entire dataset using zscore, assuming there was noise in the dataset. The dataset has been divided into two sections: train (66%) and test (33%). Five distinct models were trained, tested, and evaluated. It was discovered that the unbalanced dataset resulted in excellent accuracy but a very poor f1_macro. \n",
    "The larger classes were undersampled, while the smaller classes were oversampled. The new class samples distribution is: c0 = 1/3, c1 = 1/4, c2 = 1/5, c3 = 1/6, c4 = 1/7.\n",
    "This division still keeps the classes unbalanced, but it is much less severe than it was at the start. Furthermore, I think that this sample division is more useful for solving the problem at hand than having the same number of samples for all classes. This is because, given the nature of the problem (prediction of the share of articles), I think that the first classes will be far more populated than the last classes. Non-viral articles outnumber viral articles.\n",
    "With the new dataset, models were trained, tested, and evaluated. These demonstrate a significant improvement in f1_macro. SVC was chosen as the best model and it was tuned. Because of the computational complexity of SVC, the tuning was done on a smaller grid than I would have preferred.\n",
    "The SVM with the following parameters {'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovo', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False} is the best model for solving the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4. Summary\n",
    "Provide a summary discussion (in English) of your solution <b>(at least 500 words)</b> feel free to include plots figures and tables.\n",
    "\n",
    "<b>This is a mandatory step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution for forecasting article share is discussed in detail below. The dataset was loaded first, and then the rows with at least one of the following characteristics were deleted:\n",
    "- number of values other than the number of columns;\n",
    "- None values;\n",
    "- n/a. the values;\n",
    "- values outside the domain of the respective column.\n",
    "\n",
    "This operation removed 135 lines, resulting in a clean dataset of 39513 samples.\n",
    "As a result, the feature importance has been investigated using a RandomForestRegressor. As a result, features with an importance less than 0.02 have been removed. The remaining features after this operation were 18.\n",
    "\n",
    "Graphs were created to investigate the value distribution for each feature. Because of the non-homogeneous distribution, 1875 samples were eliminated as outliers. Because it was assumed that the dataset at our disposal could not be perfect, some noise was removed. To avoid losing important data, not too many samples were deleted.\n",
    "The final dataset contains 37638 samples and 18 features. This was divided into train sets (66%) and test sets (33%).\n",
    "\n",
    "Models have been developed, trained, and evaluated. The results show that the accuracy is excellent, but the f1 macro is low. Almost all models classify the entirety of the samples as class0, and because class0 is the majority class, the accuracy is high; however, we cannot create models that classify everything in a single class. We must develop models that are sensitive to the classification of all five classes.\n",
    "\n",
    "For studying predictions on unbalanced classes, f1 macro is a useful value. Our problem has unbalanced classes: it is more likely that an article will be little shared than that it will go viral, and there are many less famous articles than famous articles. Because the nature of the problem requires unbalanced classes, our goal is to have greater sensitivity for the majority classes while also being able to predict the other classes. As a result, we want to increase f1_macro while maintaining accuracy.\n",
    "to do this, the train set is balanced as follows:\n",
    "- class 0: 1/3 samples\n",
    "- class 1: 1/4 samples\n",
    "- class 2: 1/5 samples\n",
    "- class 3: 1/6 samples\n",
    "- class 4: 1/7 samples\n",
    "\n",
    "the distribution totals 1.0928, it is tolerable, we will have just over 2/3 of the training set.\n",
    "\n",
    "An uniform sample distribution was also tested, but in the evaluation, poor results were obtained in the majority classes, precisely because the problem has unbalanced classes by definition, but models with balanced classes were trained. Another attempt was to distribute the test set for 4/7 on the first two classes and 3/7 on the last three classes, but this resulted in a correct prediction of the first two classes and a bad prediction of the last three, reducing the problem to the prediction of only two classes. As a result, the solution's choice turned out to be the best.\n",
    "\n",
    "The models were trained and tested obtaining the following results:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> name </th><th>accuracy</th><th>f1macro</th><th>cv_accuracy</th><th>cv_f1macro</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DT</td><td>0.49</td><td>0.20</td><td>0.4071 (+/- 0.3962)</td><td>0.1513 (+/- 0.1054)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVM</td><td>0.65</td><td>0.25</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BOOST</td><td>0.59</td><td>0.23</td><td>0.7957 (+/- 0.0037)</td><td>0.1783 (+/- 0.0023)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RND_FOREST</td><td>0.70</td><td>0.26</td><td>0.6614 (+/- 0.5165)</td><td>0.1570 (+/- 0.1051)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MLPN</td><td>0.65</td><td>0.22</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Due to the obvious presence of better models, DT and BOOST were discarded. RND FOREST performed well on the test set but was rejected due to insufficient crossvalidation results (the crossvalidation values had a greater weight in my evaluation). Both the MLPN and SVM models have been fine-tuned (with half of the test set). Model overfitting was caused by MLPN tuning, whereas SVM tuning did not underperform or outperform the model results prior to tuning. Due to MLPN's overfitting flaw, SVM was presented as the best model ('SVM_accuracy' in `tuned_models`).\n",
    "\n",
    "It should be noted that the test set was only used to evaluate the model and was not altered or balanced in any way. If the test set had been balanced in the same way that the train set was, I believe the results would have been significantly better. However, this behavior would have compromised the solution's integrity. It is also important to note that the train set after sample redistribution remains 2/3 of the total set, allowing for a correct crossvalidation result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
